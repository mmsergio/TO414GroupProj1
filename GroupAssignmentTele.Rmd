title: "HW6 Telemarketing"
author: "Group"
date: "3/22/2020"
output:
  html_document:
    toc: true
    theme: readable
    highlight: tango
    code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
Describe as-is state and to be state 

## Downloading and Prepping the Data

```{r}
library(neuralnet)
library(gmodels)
library(caret)
library(class)

#Downloading and Prepping the Data
tele <- read.csv("tele.csv", stringsAsFactors = TRUE)
summary(tele)

#We are deleting the "duration" variable because it is an after the fact measurement. We only should be using variables that we know before the call
tele$duration <- NULL

# Deleting the column X
tele$X <- NULL

# Changing pdays to a dummy and deleting pdays
tele$pdaysdummy <- ifelse(tele$pdays == 999, 0, 1)
tele$pdays <- NULL

str(tele)
summary(tele)
```

## Getting Data Ready for Analysis

```{r}
# Using model.matrix to convert all the factors to dummy variables
# We are converting all of the factors into dummy variables as the input into knn has to be numeric

telemm <- as.data.frame(model.matrix(~.-1,tele))
str(telemm)

# Randomize the rows in the data (shuffling the rows)
set.seed(12345)
tele_random <- telemm[sample(nrow(telemm)),]

#Normalize the data
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

# we are going to normalize everything 
tele_norm <- as.data.frame(lapply(tele_random, normalize))
```

## Clustering the data

```{r}
tele_z <- as.data.frame(lapply(tele_norm, scale))
tele_z$yyes <- NULL
  #We use normalized so the data set is numeric
set.seed(12345)
tele_clusters <- kmeans(tele_z, 5)

tele_clusters$centers 

tele_norm$cluster <- tele_clusters$cluster
  #Cluster ID into the normalized data set

aggregate(data = tele_norm, yyes ~ cluster, mean)
  #Cluster 3 and 4 no need to build a regression, already reach the 16% threshold
length(which(tele_norm$cluster == 3))
length(which(tele_norm$cluster == 4))
#Creating 3 new data sets for each cluster

tele_norm_cl1 <- subset(tele_norm, cluster == 1)
tele_norm_cl2 <- subset(tele_norm, cluster == 2)
tele_norm_cl5 <- subset(tele_norm, cluster == 5)
```

## Building a Logistic Regression

```{r}
#For those in cluster 1
set.seed(12345)
test_cl1 <- sample(1:nrow(tele_norm_cl1), 2500)
tele_norm_cl1_train <- tele_norm_cl1[-test_cl1,]
tele_norm_cl1_test <- tele_norm_cl1[test_cl1,]
  #We create the test and train data set

reg1 <- glm(yyes ~ ., data = tele_norm_cl1_train)
reg1 <-  step(reg1, direction = "backward")
  #Optimizing the regression

tele_norm_cl1_test$yyes <- NULL
tele_norm_cl1_test$yyes1 <- predict(reg1, newdata = tele_norm_cl1_test, type = "response")
tele_norm_cl1_test$yyes1 <- ifelse(tele_norm_cl1_test$yyes1 > 0.5, 1, 0)
  #Create yyes column in test data set with the predictions from the regression

tele_norm_cl1_testlabels <- tele_norm_cl1[test_cl1, "yyes"]

CrossTable(x = tele_norm_cl1_testlabels, y = tele_norm_cl1_test$yyes1, prop.chisq=FALSE)




#For those in cluster 2
set.seed(12345)
test_cl2 <- sample(1:nrow(tele_norm_cl2), 3000)
tele_norm_cl2_train <- tele_norm_cl2[-test_cl2,]
tele_norm_cl2_test <- tele_norm_cl2[test_cl2,]

reg2 <- glm(yyes ~ ., data = tele_norm_cl2_train)
reg2 <-  step(reg2, direction = "backward")
  #Optimizing

tele_norm_cl2_test$yyes <- NULL
tele_norm_cl2_test$yyes1 <- predict(reg2, newdata = tele_norm_cl2_test, type = "response")
tele_norm_cl2_test$yyes1 <- ifelse(tele_norm_cl2_test$yyes1 > 0.5, 1, 0)

tele_norm_cl2_testlabels <- tele_norm_cl2[test_cl2, "yyes"]

CrossTable(x = tele_norm_cl2_testlabels, y = tele_norm_cl2_test$yyes1, prop.chisq=FALSE)




#For those in cluster 5
set.seed(12345)
test_cl5 <- sample(1:nrow(tele_norm_cl5), 1100)
tele_norm_cl5_train <- tele_norm_cl5[-test_cl5,]
tele_norm_cl5_test <- tele_norm_cl5[test_cl5,]

reg5 <- glm(yyes ~ ., data = tele_norm_cl5_train)
reg5 <-  step(reg5, direction = "backward")

tele_norm_cl5_test$yyes <- NULL
tele_norm_cl5_test$yyes1 <- predict(reg5, newdata = tele_norm_cl5_test, type = "response")
tele_norm_cl5_test$yyes1 <- ifelse(tele_norm_cl5_test$yyes1 > 0.55, 1, 0)

tele_norm_cl5_testlabels <- tele_norm_cl5[test_cl5, "yyes"]

CrossTable(x = tele_norm_cl5_testlabels, y = tele_norm_cl5_test$yyes1, prop.chisq=FALSE)

```



## Building a KNN Model

```{r}
#CLUSTER 1
tele_norm_cl1_test$yyes3 <- NULL
tele_norm_cl1_test$yyes4 <- NULL
tele_norm_cl1_test$yyes2 <- NULL
  #Debbuging purposes
tele_norm_cl1_train$yyes <- NULL
tele_norm_cl1_trainlabels <- tele_norm_cl1[-test_cl1, "yyes"]
set.seed(12345)
tele_norm_cl1_knnprediction <- knn(train = tele_norm_cl1_train, test = tele_norm_cl1_test[-tele_norm_cl1_test$yyes1],
                      cl = tele_norm_cl1_trainlabels, k=4)
CrossTable(x = tele_norm_cl1_testlabels, y = tele_norm_cl1_knnprediction, prop.chisq=FALSE)

tele_norm_cl1_test$yyes2 <- tele_norm_cl1_knnprediction

#CLUSTER 2
tele_norm_cl2_train$yyes <- NULL
tele_norm_cl2_test$yyes2 <- NULL
tele_norm_cl2_test$yyes3 <- NULL
tele_norm_cl2_test$yyes4 <- NULL
tele_norm_cl2_trainlabels <- tele_norm_cl2[-test_cl2, "yyes"]
set.seed(12345)
tele_norm_cl2_knnprediction <- knn(train = tele_norm_cl2_train, test = tele_norm_cl2_test[-tele_norm_cl2_test$yyes1],
                      cl = tele_norm_cl2_trainlabels, k=5)
CrossTable(x = tele_norm_cl2_testlabels, y = tele_norm_cl2_knnprediction, prop.chisq=FALSE)
  #Again very bad results
tele_norm_cl2_test$yyes2 <- tele_norm_cl2_knnprediction

#CLUSTER 5
tele_norm_cl5_train$yyes <- NULL
tele_norm_cl5_test$yyes2 <- NULL
tele_norm_cl5_test$yyes3 <- NULL
tele_norm_cl5_test$yyes4 <- NULL
tele_norm_cl5_trainlabels <- tele_norm_cl5[-test_cl5, "yyes"]
set.seed(12345)
tele_norm_cl5_knnprediction <- knn(train = tele_norm_cl5_train, test = tele_norm_cl5_test[-tele_norm_cl5_test$yyes1],
                      cl = tele_norm_cl5_trainlabels, k=6)
CrossTable(x = tele_norm_cl5_testlabels, y = tele_norm_cl5_knnprediction, prop.chisq=FALSE)
tele_norm_cl5_test$yyes2 <- tele_norm_cl5_knnprediction
```


## Building a Neural Net 

```{r}
tele_norm_cl1_train$yyes <- tele_norm_cl1_trainlabels
  #Restore the yyes column
set.seed(12345)
neuralmod_cl1 <- neuralnet(yyes~., data=tele_norm_cl1_train, hidden = c(3,1))

tele_norm_cl1_test$yyes3 <- predict(neuralmod_cl1, tele_norm_cl1_test)
tele_norm_cl1_test$yyes3 <- ifelse(tele_norm_cl1_test$yyes3 > 0.7, 1, 0)
  #Add column with prediction

CrossTable(x = tele_norm_cl1_testlabels, y = tele_norm_cl1_test$yyes3, prop.chisq=FALSE)

confusionMatrix(as.factor(tele_norm_cl1_test$yyes3), as.factor( tele_norm_cl1_testlabels), positive = "1")
  #Notice that is inverted (compared to the Cross Table)




#CLUSTER 2
tele_norm_cl2_train$yyes <- tele_norm_cl2_trainlabels
set.seed(12345)
neuralmod_cl2 <- neuralnet(yyes~., data=tele_norm_cl2_train)

tele_norm_cl2_test$yyes3 <- predict(neuralmod_cl2, tele_norm_cl2_test)
tele_norm_cl2_test$yyes3 <- ifelse(tele_norm_cl2_test$yyes3 > 0.3, 1, 0)

CrossTable(x = tele_norm_cl2_testlabels, y = tele_norm_cl2_test$yyes3, prop.chisq=FALSE)

confusionMatrix(as.factor(tele_norm_cl2_test$yyes3), as.factor(tele_norm_cl2_testlabels), positive = "1")
    #VERY BAD, THE 2 CLUSTER PROBABLY SHOULD NOT BE CALLED
  #Adding more hidden layers and nodes does not improve that much, so I have not included them



#CLUSTER 5
tele_norm_cl5_train$yyes <- tele_norm_cl5_trainlabels
set.seed(12345)
neuralmod_cl5 <- neuralnet(yyes~., data=tele_norm_cl5_train, hidden = c(2,1))

tele_norm_cl5_test$yyes3 <- predict(neuralmod_cl5, tele_norm_cl5_test)
tele_norm_cl5_test$yyes3 <- ifelse(tele_norm_cl5_test$yyes3 > 0.5, 1, 0)

CrossTable(x = tele_norm_cl5_testlabels, y = tele_norm_cl5_test$yyes3, prop.chisq=FALSE)

confusionMatrix(as.factor(tele_norm_cl5_test$yyes3), as.factor(tele_norm_cl5_testlabels), positive = "1")

  #Improved 
```



## Building a Join Prediction
```{r}
#Now we will use the predictions from the previous tests to build a joint prediction

#CLUSTER 1
tele_norm_cl1_test$yyes4 <- NULL
  #For debugging purposes
tele_norm_cl1_test$yyes4 <- ifelse((as.numeric(as.character(tele_norm_cl1_test$yyes1)) + 
                                     as.numeric(as.character(tele_norm_cl1_test$yyes2)) + 
                                     as.numeric(as.character(tele_norm_cl1_test$yyes3))) > 1, 1, 0 )

  #If the sum of the three predictions is more than 1, it means that two or more of those three predictors is one, and thus, we get a 1 in the final prediction

CrossTable(x = tele_norm_cl1_testlabels, y = tele_norm_cl1_test$yyes4, prop.chisq=FALSE)



#CLUSTER 2
tele_norm_cl2_test$yyes4 <- NULL
tele_norm_cl2_test$yyes4 <- ifelse((as.numeric(as.character(tele_norm_cl2_test$yyes1)) + 
                                     as.numeric(as.character(tele_norm_cl2_test$yyes2)) + 
                                     as.numeric(as.character(tele_norm_cl2_test$yyes3))) > 1, 1, 0 )

CrossTable(x = tele_norm_cl2_testlabels, y = tele_norm_cl2_test$yyes4, prop.chisq=FALSE)



#CLUSTER 5
tele_norm_cl5_test$yyes4 <- NULL
tele_norm_cl5_test$yyes4 <- ifelse((as.numeric(as.character(tele_norm_cl5_test$yyes1)) + 
                                     as.numeric(as.character(tele_norm_cl5_test$yyes2)) + 
                                     as.numeric(as.character(tele_norm_cl5_test$yyes3))) > 1, 1, 0 )

CrossTable(x = tele_norm_cl5_testlabels, y = tele_norm_cl5_test$yyes4, prop.chisq=FALSE)

```

## Conclusion and profitability analysis

  Firstly, given that a call costs 1 dollar and a successful call yields 6 dollars as revenue, 1/6 calls need to be successful to cover expenses, therefore the break-even point is going to be established in 16%. Moreover, there are no fixed costs so the average cost is going to equal the variable cost (price of making one call) and there is no need to take advantage of economies of scale or massively call to lower the average cost.That being said, the highest profit is going result from the highest repurchase ratio (true positives over total calls).
  
  After successfully clustering the data, two of the clusters exceed the break-even point (16%). As a consequence, these two groups of customers are profitable, and all clients within them should be called as more than 16% of them will be willing to repurchase. These two clusters are shown as number 3 and 4, with a repurchase rate of 20% and 64% respectively. Regarding the profitability of massively calling the clients in Cluster 3, 11519 calls would be performed with a total cost of $ 11519, but given than 20% are going to be successful, this leads to a total revenue of 13823 and a profit of 2304. Secondly, only 1518 call are needed to fully cover cluster 4 and with a 64% repurchase rate, the revenues are 5829, resulting in 4311 of profit. 
  
  Next, the other three clusters (1,2 and 5) remain unprofitable, so now we need to predict who to call within these Clusters to have a successful sale. The KNN prediction is almost useless in this situation: as in these clusters very few people actually buy, the prediction will virtually every time yield 0 (client does not repurchase) due to the soaring number of neighbors who do not buy. Consequently, the joint prediction is going to be greatly influenced by KNN results which are not accurate enough, questioning also the validity of this prediction. 
  
  All of our predictions have been created to maximize its precision. But one of the most critical issues, is the high number of false negatives (potential clients that we do not identify as such) that we have in all of the models (very low sensitivity). But trying to have those clients as true positives, increases the number of false positives as well, and reduces the successful calls over total calls ratio. As making more calls does not reduce the average cost, an effort to capture those false negatives reduces our precision and thus profitability.
  
  Mention that cluster 2 should not be called as it is not worth it. 









