---
title: "Project 2: Telecon Business"
author: "Group 14"
date: '2022-10-30'
output: 
  html_document:
    toc: true
    toc_float: true
    highlight: tango
    theme: united
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Introduction and business problem

   The selected data sets displays information about the client profile of customers in a telecommunications firm. This information includes demographic data about the client, data regarding payments and billing, information about the services that the client has received and finally, whether the client has renewed its contract with the firm or not (Churn variable). In this scenario the Churn variable is our response variable and we will analyze what factors affect it the most. An appropriate treatment of this information is vital to any telecommunications company as it has the potential to point out how to boost retention rate. Interesting business questions that arise given the dataset include: 

Questions for the first part:

  - Improve the profitability of the Telecon: a financial problem
  
Questions for the second part:

  - Which profile are we retaining and who is leaving the subscription? (Comparison among clusters and profiles) 
  
  - What factors tend to make clients decided NOT to renew their subscription?
  
  - Is there any particular client profile that is worth and feasible to target? (Best cluster in retention rate) 
  
  - If so, can we identify loyal clients within those profiles (or clusters)? (Run predictions on 1 or 2 best clusters)


URL to dataset: https://www.kaggle.com/datasets/blastchar/telco-customer-churn


# Part 1

## Financials in the base scenario

Assumptions taken into account:

  In this case, we know both the number of clients in months 1 & 2 and the monthly charges for the services. Based on this, we can obtain the revenue of the company for these two months. Obviously, revenue decreases in Month 2 because there are less clients, some will leave the company, and our case does not take into account new clients subscribing to the company at the begining of Month 2. 

  What we don't know, though, is the costs of the company, both fixed and variable costs. These may include the fixed cost of property plant and equipment, wages, as well as Research and Development, and variable cost of providing the service to the individual. The assumption of the costs will be based on the profit margin of a healthy telecommunications company, which, according to Investopedia, is around 12,5% in the telecommunication sector.

  During Month 1, it will be assumed that the profit margin of the company is around 12,5%. The fixed and variable costs can be translated to the Month 2 (it will be the same) to calculate the new margin. Given that the telecom industry is very capital intensive, fixed costs are assumed to be high (230,000), while variable costs are around 6 times less than the average amount charged for the service (average = 64.76). When it comes to the promotion costs, these are punishing for the company, as they are losing potential revenue, but at the same time they increase the retention rate of the clients, so these are assumed to be 14.

  It is assumed as well that the company is pushing this promotion to all clients regardless on whether the are going to renew or not. So offering the promotion to clients who are not willing to renovate next month is a waste of money for the firm. The aim of the first part of the project is to improve profitability by identifying which clients are going to stay the next month so the promotion can be targeted to them. It is key to note that if a client is willing to remain subscribed, but we don't send the promotion to them, they are going to leave the firm next month. 
  
  
The breakdown of the calculation is below:

### Month 1:

  Clients = 7000
  
  **Revenues** = 453,320
  
  Monthly fixed costs (PPE, depreciation, SG&A, R&D) = 230,000
  
  Variable costs of supplying clients with services = 10.1
  Total monthly variable costs without the promotion = 70,700
  
  Variable costs of engaging a client in consumer retention promotions = 14
  Total cost of massive promotion = 98,000
  
  Total monthly variable costs = 168,700
  
  **Total costs** = 398,700 
  
  **Profit** = 54620
  
  Profit margin = 12.05%
  
### Month 2:

  Clients = 5174
  
  **Revenues** = 335,068.2

  Monthly fixed costs (PPE, depreciation, SG&A, R&D) = 230,000
  
  Variable costs of supplying clients with services = 10.1
  Total monthly variable costs = 52,257.4
  
  **Total costs** = 282,257.4
  
  **Profit** = 52810.8
  
  Profit margin = 15.76%


TOTAL PROFITS IN THIS SCENARIO: 107,430.8



Now we will run our prediction models:


```{r}
clients <- read.csv("ClientsChurn.csv")
totalrevenues = sum(clients$MonthlyCharges)
totalrevenues
totalcost = 299000
aggregate(MonthlyCharges ~ Churn, data = clients, sum)
aggregate(MonthlyCharges ~ Churn, data = clients, mean)
mean(clients$MonthlyCharges)
nrow(clients[clients$Churn==0, ])
nrow(clients[clients$Churn == 1,])
```


## Data cleaning

```{r}
library(neuralnet)
library(gmodels)
library(caret)
library(class)
library(corrplot)
library(C50)
library(kernlab) 

clients <- read.csv("ClientsChurn.csv")
str(clients)
summary(clients)
  #Make factors
  #We need to decide how to deal with the NAs in the Total Charges column


clients$customerID <- NULL
  #ID is not needed
clients$gender <- as.factor(clients$gender)
clients$SeniorCitizen <- as.factor(clients$SeniorCitizen)

clients$Partner <- ifelse(clients$Partner == "Yes", 1, 0)
clients$Partner <- as.factor(clients$Partner)

clients$Dependents <- ifelse(clients$Dependents == "Yes", 1, 0)
clients$Dependents <- as.factor(clients$Dependents)

clients$PhoneService <- ifelse(clients$PhoneService == "Yes", 1, 0)
clients$PhoneService <- as.factor(clients$PhoneService)

clients$MultipleLines <- as.factor(clients$MultipleLines)
clients$InternetService <- as.factor(clients$InternetService)
clients$OnlineSecurity <- as.factor(clients$OnlineSecurity)
clients$OnlineBackup <- as.factor(clients$OnlineBackup)
clients$DeviceProtection <- as.factor(clients$DeviceProtection)
clients$TechSupport <- as.factor(clients$TechSupport)
clients$StreamingMovies <- as.factor(clients$StreamingMovies)
clients$StreamingTV <- as.factor(clients$StreamingTV)
clients$Contract <- as.factor(clients$Contract)

clients$PaperlessBilling <- ifelse(clients$PaperlessBilling == "Yes", 1, 0)
clients$PaperlessBilling <- as.factor(clients$PaperlessBilling)

clients$Churn <- ifelse(clients$Churn == "Yes", 1, 0)
clients$Churn <- as.factor(clients$Churn)

clients$PaymentMethod <- as.factor(clients$PaymentMethod)

clients$TotalCharges <- ifelse(is.na(clients$TotalCharges), median(clients$TotalCharges, na.rm = TRUE), clients$TotalCharges)
  #The NA values (a very small number) have been converted to the median

str(clients)
summary(clients)
  #All the variables but the numeric ones are made factors. Binary variables (yes or no) have been converted to 1 and 0

```

## Normalization and test/train dataframes
```{r}
clientss <- as.data.frame(model.matrix(~.-1,clients))
  #Factors are turned into dummy variables

normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

clients_norm <- as.data.frame(lapply(clientss, normalize))
  #Numeric variables are normalized

set.seed(579)
test <- sample(1:nrow(clients_norm), 2000)
  #Test made of 28% of all observations

clients_train <- clients_norm[-test,]
clients_test <- clients_norm[test,]

clients_testlabels <- clients_norm[test, "Churn1"]
clients_trainlabels <- clients_norm[-test, "Churn1"]

  #Train and test data frames created. Labels for future use. 
```

## Buildign a Logistic Regression

```{r}
reg1 <- glm(Churn1 ~ ., data = clients_train, family = "binomial")
reg1 <- step(reg1, direction = "backward")

lrprediction <- predict(reg1, newdata = clients_test, type = "response")
lrprediction <- ifelse(lrprediction > 0.5, 1, 0)

confusionMatrix(as.factor(lrprediction), as.factor(clients_test$Churn1), positive = "1")

lrprediction <- predict(reg1, newdata = clients_test, type = "response")

  #Original values are restored to build the stacked model afterwards
```

## Building a KNN prediction 

```{r}
clients_train$Churn1 <- NULL
clients_test$Churn1 <- NULL

set.seed(579)
KNNprediction <- knn(train = clients_train, test = clients_test,
                      cl = clients_trainlabels, k=sqrt(nrow(clients_train)))

CrossTable(x = clients_testlabels,y= KNNprediction,prop.chisq=FALSE)
confusionMatrix(as.factor(KNNprediction), as.factor(clients_testlabels))

  # Kappa = 0.46

clients_train$Churn1 <- clients_trainlabels
clients_test$Churn1 <- clients_testlabels
  #Churn columns is restored
```

## Neural Net

```{r}
set.seed(579)
neural_model <- neuralnet(Churn1~., data=clients_train)
ANNprediction <- predict(neural_model, newdata = clients_test)

ANNprediction <- ifelse(ANNprediction > 0.5, 1, 0)

CrossTable(x = ANNprediction, y = clients_test$Churn, prop.chisq=FALSE)
confusionMatrix(as.factor(ANNprediction), as.factor(clients_testlabels), positive = "1")

  # Kappa = 0.48

ANNprediction <- predict(neural_model, newdata = clients_test)
  # Original values restored to create the stacked model
```

## Building a Decision Tree

```{r}
treemod <- C5.0(as.factor(Churn1) ~., data = clients_train)
treepred <- predict(treemod, clients_test)

plot(treemod)

confusionMatrix(as.factor(treepred), as.factor(clients_testlabels))
  # Kappa = 0.43
```

## Building a SVM prediction

```{r}
SVM_model <- ksvm(as.factor(Churn1)~., data = clients_train, kernel = "vanilladot")
SVMprediction <- predict(SVM_model, clients_test)

confusionMatrix(as.factor(SVMprediction), as.factor(clients_testlabels))

  #Kappa = 0.47
```

## Buidling the stacked model
```{r}
allpredictions <- data.frame(lrprediction, KNNprediction, ANNprediction, SVMprediction, treepred, clients_testlabels)
summary(allpredictions)

colnames(allpredictions)[colnames(allpredictions) == "clients_testlabels"] ="Churn"
  #Dataset with all 5 predictions and the lables that contain the true values for the test dataset
set.seed(5948)
combined_test_set <- sample(1:nrow(allpredictions), 0.3*nrow(allpredictions)) 

combined_train <- allpredictions[-combined_test_set, ]
combined_test <- allpredictions[combined_test_set,]

combined_dtree <- C5.0(as.factor(Churn) ~., data = combined_train)
plot(combined_dtree)

combined_pred <- predict(combined_dtree, combined_test)

confusionMatrix(as.factor(combined_pred), as.factor(combined_test$Churn))

```

## Stacked model with cost matrix

Now we will include a cost matrix to reduce the false positives. These false positives (we predict 1 when they are actually 0) are those clients that are initially willing to stay with the company but through our model, we predict that they are going to leave the firm so we do not bother to send the promotion to them. But as a consequence of not sending them the promotion, they will leave (we established this in the assumptions).

```{r}
costmatrix <- matrix(c(0,2,1,0), nrow = 2)

combined_dtree2 <- C5.0(as.factor(Churn) ~ ., data = combined_train, costs = costmatrix)
combined_pred2 <- predict(combined_dtree2, combined_test)

confusionMatrix(as.factor(combined_pred2), as.factor(combined_test$Churn), positive = "1")
```

At the expense of sacrificing some accuracy, we manage to retain 8 extra clients that we initially identified they were leaving the firm (Churn 1) but they were in fact loyal customers (Churn 0). Initially, these 8 clients would not receive the promotion, but now they do, and will remain next month. Nevertheless, 16 clients that previously we correctly identified that they were going to leave the company (True Positives), now we predict they will remain (16 more false negatives). So we will send 16 additional promotions which are going to be worthless. Let's analayze the profitability change of adding the costs Matrix:

16 + 8 = 24 additional promotions are sent                            24 * 14 = $336 additional costs during Month 1
We get 8 more clients during Month 2                                  8 * 64.76 = $518.08 additional revenue

As a conclusion, lowering the accuracy to capture those false positives has been profitable: the revenues during Month 2 of retaining those clients is way greater than the costs of sending more promotions during Month 1. 


## Comparison of Kappa and accuracy

The following section contains the values of accuracy and Kappa for each of rhe models

|               | Logistic Reg   | KNN      | Neural Net  |    SVM     |   Decision Tree | Stacked Model | Stacked Model with costs |
|:--------------|:--------------:|:--------:|:-----------:|:----------:|:---------------:|:-------------:|:------------------------:|
|   Kappa       |  0.48          |0.45      | 0.48        |     0.47   |    0.43         | 0.41          |            0.33          |
|  Accuracy     |  0.8           |  0.79    |   0.81      | 0.8        | 0.8             | 0.80          |     0.79                 |


## Profitability in the Second Scenario (predictions implemented)

Now we will analyze the profitability of the company if a prediction model was used during Month 1 to identify clients willing to stay and target the promotion to them, rather than massively sending it to all the clients. 

First we will scale the results to our original dataset. In our Stacked model with costs we have 600 clients, so the original dataset is 11.67 times greater. In the stacked model we send 523 promotions, 425 to clients who are actually going to stay and 98 to clients we have misidentified and will leave the company; this translates into 4958 successful promotions and 1144 wasted promotions. Also, in the prediction model, 27 clients who initially would remain in the company are not identified as loyal and do not receive the promotion, so they will end up leaving. In the original dataset that would be 315 lost clients. Lastly, the model has 50 true positives, clients that did not intend to renew so we do not send them the promotion, correctly. These would be 583 clients in the original dataset. Once the model has been scaled we can calculate the financials of the new scenario:


### Month 1: 

  Clients = 7000
  
  **Revenues** = 453,320
  
  Monthly fixed costs (PPE, depreciation, SG&A, R&D) = 230,000
  
  Variable costs of supplying clients with services = 10.1
  Total monthly variable costs without the promotion = 70,700
  
  Variable costs of engaging a loyal client in consumer retention promotions = 14
  Total cost of selective promotion = 14 * 4958 = 69,412
  
  Total monthly variable costs = 140,112
  
  **Total costs** = 370,112
  
  **Profit** = 83208
  
  Profit margin = 18.35%
  
  
### Month 2:

  Clients = 4958 (only those clients with Churn 0 that we sent the promotion to)
  
  **Revenues** = 321,080.1

  Monthly fixed costs (PPE, depreciation, SG&A, R&D) = 230,000
  
  Variable costs of supplying clients with services = 10.1
  Total monthly variable costs = 50,075.8
  
  **Total costs** = 280,075.8
  
  **Profit** = 41,004.3
  
  Profit margin = 12.77%

TOTAL PROFITS IN THIS SCENARIO: 124,212.3


When it comes to compare the results we seen an increase in profitability during Month 1 as less promotions are sent and thus the total costs are significantly reduced. But obviously during Month 2, we have less revenues as the prediction models are not able to capture all the clients that were willing to stay, so some did not receive the promotion and ended up leaving. 

Actually, costs have been reduced during Month 1 by \$28,588 (398700-370112) and revenues decreased during Month 2 by \$13988.1 (335068.2-321080.1) so overall it can be said that implementing the prediction models has boosted the financial of the firm. Also the firm incurs in a lower cost during Month 2 as there are less clients, \$2181.6 less. 
In fact profits in the company have increased by \$16781.5 (124212.3 - 107430.8).(It can also be computed like this: 28588 - 13988.1 + 2181.6)

# Part 2

## Clustering of clients

```{r}
set.seed(7593)
clients_z <- as.data.frame(lapply(clients_norm, scale))

clients_clusters <- kmeans(clients_norm, 5)

clients_z$Churn1 <- NULL
clients_clusters <- kmeans(clients_z, 5)

clients_norm$cluster <- clients_clusters$cluster

clients_clusters$centers

aggregate(data = clients_norm, Churn1 ~ cluster, mean)
  #We also need to interpret this, ask in class
```

## Cluster Data Splitting

```{r}
#Create models on two highest clusters
cluster4 <- clients_norm[ clients_norm$cluster == 4, ]
cluster4set <- sample(1:nrow(cluster4), 0.8*nrow(cluster4))
cluster4train <- cluster4[cluster4set, ]
cluster4test <- cluster4[-cluster4set, ]
cluster4train$cluster <- NULL
cluster4test$cluster <- NULL

cluster3 <- clients_norm[ clients_norm$cluster == 3, ]
cluster3set <- sample(1:nrow(cluster3), 0.8*nrow(cluster3))
cluster3train <- cluster3[cluster3set, ]
cluster3test <- cluster3[-cluster3set, ]
cluster3train$cluster <- NULL
cluster3test$cluster <- NULL
```

## Creating Models for Cluster Analysis

```{r}
#logistic regression
logreg4 <- glm(Churn1 ~ ., data = cluster4train, family = "binomial")
logreg4 <- step(logreg4, direction = "backward")

log4prediction <- predict(logreg4, newdata = cluster4test, type = "response")
log4prediction <- ifelse(log4prediction > 0.5, 1, 0)

confusionMatrix(as.factor(log4prediction), as.factor(cluster4test$Churn1), positive = "1")

lrprediction <- predict(logreg4, newdata = cluster4test, type = "response")

logreg3 <- glm(Churn1 ~ ., data = cluster3train, family = "binomial")
logreg3 <- step(logreg3, direction = "backward")

log3prediction <- predict(logreg3, newdata = cluster3test, type = "response")
log3prediction <- ifelse(log3prediction > 0.5, 1, 0)

confusionMatrix(as.factor(log3prediction), as.factor(cluster3test$Churn1), positive = "1")

lrprediction <- predict(logreg3, newdata = cluster3test, type = "response")
```

```{r}
## knn
clients4_testlabels <- cluster4test$Churn1
clients4_trainlabels <- cluster4train$Churn1
clients3_testlabels <- cluster3test$Churn1
clients3_trainlabels <- cluster3train$Churn1
cluster4train$Churn1 <- NULL
cluster4test$Churn1 <- NULL
cluster3train$Churn1 <- NULL
cluster3test$Churn1 <- NULL

set.seed(579)
KNNprediction4 <- knn(train = cluster4train, test = cluster4test,
                      cl = clients4_trainlabels, k=sqrt(nrow(cluster4train)))

CrossTable(x = clients4_testlabels,y= KNNprediction4,prop.chisq=FALSE)
confusionMatrix(as.factor(KNNprediction4), as.factor(clients4_testlabels))

KNNprediction3 <- knn(train = cluster3train, test = cluster3test,
                      cl = clients3_trainlabels, k=sqrt(nrow(cluster3train)))

CrossTable(x = clients3_testlabels,y= KNNprediction3,prop.chisq=FALSE)
confusionMatrix(as.factor(KNNprediction3), as.factor(clients3_testlabels))
  # Kappa = 

cluster4train$Churn1 <- clients4_trainlabels
cluster4test$Churn1 <- clients4_testlabels
cluster3train$Churn1 <- clients3_trainlabels
cluster3test$Churn1 <- clients3_testlabels
  #Churn columns is restored
```

```{r}
#neuralnet
neural_model4 <- neuralnet(Churn1~., data=cluster4train)
ANNprediction4 <- predict(neural_model4, newdata = cluster4test)

ANNprediction4 <- ifelse(ANNprediction4 > 0.5, 1, 0)

CrossTable(x = ANNprediction4, y = cluster4test$Churn, prop.chisq=FALSE)
confusionMatrix(as.factor(ANNprediction4), as.factor(clients4_testlabels), positive = "1")

  # Kappa = 

ANNprediction3 <- predict(neural_model4, newdata = cluster4test)

neural_model3 <- neuralnet(Churn1~., data=cluster3train)
ANNprediction3 <- predict(neural_model3, newdata = cluster3test)

ANNprediction3 <- ifelse(ANNprediction3 > 0.5, 1, 0)

CrossTable(x = ANNprediction3, y = cluster3test$Churn, prop.chisq=FALSE)
confusionMatrix(as.factor(ANNprediction3), as.factor(clients3_testlabels), positive = "1")

  # Kappa = 

ANNprediction3 <- predict(neural_model3, newdata = cluster3test)
```

```{r}
#trees
treemod4 <- C5.0(as.factor(Churn1) ~., data = cluster4train)
treepred4 <- predict(treemod4, cluster4test)

plot(treemod4)

confusionMatrix(as.factor(treepred4), as.factor(clients4_testlabels))
  # Kappa = 
treemod3 <- C5.0(as.factor(Churn1) ~., data = cluster3train)
treepred3 <- predict(treemod3, cluster3test)

plot(treemod3)

confusionMatrix(as.factor(treepred3), as.factor(clients3_testlabels))
  # Kappa = 
```

```{r}
#SVM
SVM_model4 <- ksvm(as.factor(Churn1)~., data = cluster4train, kernel = "vanilladot")
SVMprediction4 <- predict(SVM_model4, cluster4test)

confusionMatrix(as.factor(SVMprediction4), as.factor(clients4_testlabels))

  #Kappa = 
SVM_model3 <- ksvm(as.factor(Churn1)~., data = cluster3train, kernel = "vanilladot")
SVMprediction3 <- predict(SVM_model3, cluster3test)

confusionMatrix(as.factor(SVMprediction3), as.factor(clients3_testlabels))

  #Kappa = 
```
## Creating Two Stacked Models For the Clusters

```{r}
#Tree cluster 4
allpredictions4 <- data.frame(log4prediction, KNNprediction4, ANNprediction4, treepred4, clients4_testlabels)
summary(allpredictions4)

colnames(allpredictions4)[colnames(allpredictions4) == "clients4_testlabels"] ="Churn"
  #Dataset with all 5 predictions and the lables that contain the true values for the test dataset
set.seed(5948)
combined_test_set4 <- sample(1:nrow(allpredictions4), 0.3*nrow(allpredictions4)) 

combined_train4 <- allpredictions4[-combined_test_set4, ]
combined_test4 <- allpredictions4[combined_test_set4,]

combined_dtree4 <- C5.0(as.factor(Churn) ~., data = combined_train4)
plot(combined_dtree4)

combined_pred4 <- predict(combined_dtree4, combined_test4)

confusionMatrix(as.factor(combined_pred4), as.factor(combined_test4$Churn))

# tree cluster 3
allpredictions3 <- data.frame(log3prediction, KNNprediction3, ANNprediction3, treepred3, clients3_testlabels)
summary(allpredictions3)

colnames(allpredictions3)[colnames(allpredictions3) == "clients3_testlabels"] ="Churn"
  #Dataset with all 5 predictions and the lables that contain the true values for the test dataset
set.seed(5948)
combined_test_set3 <- sample(1:nrow(allpredictions3), 0.3*nrow(allpredictions3)) 

combined_train3 <- allpredictions3[-combined_test_set3, ]
combined_test3 <- allpredictions3[combined_test_set3,]

combined_dtree3 <- C5.0(as.factor(Churn) ~., data = combined_train3)
plot(combined_dtree3)

combined_pred3 <- predict(combined_dtree3, combined_test3)

confusionMatrix(as.factor(combined_pred3), as.factor(combined_test3$Churn))

```

- Which profile are we retaining and who is leaving the subscription? (Comparison among clusters and profiles)


- What factors tend to make clients decided NOT to renew their subscription?
  
  
- Is there any particular client profile that is worth and feasible to target? (Best cluster in retention rate) 
  

- If so, can we identify loyal clients within those profiles (or clusters)?

