---
title: "Project2"
author: "Group 14"
date: '2022-10-30'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Introduction and business problem

    The selected data sets displays information about the client profile of customers in a telecommunications firm. This information includes demographic data about the client, data regarding payments and billing, information about the services that the client has received and finally, whether the client has renewed its contract with the firm or not (Churn variable). In this scenario the Churn variable is our response variable and we will analyze what factors affect it the most. An appropriate treatment of this information is vital to any telecommunications company as it has the potential to point out how to boost retention rate. Interesting business questions that arise given the dataset include: 
    
  - Which profile are we retaining and who is leaving the subscription? 
  - How can we improve the retention rate? 
  - Is there any way to increase consumer loyalty? 
  - Is there any particular client profile that is worth and feasible to target?  
  - Can we determine if a recent client is going to renovate its subscription once he is in our company? How can we make the client do so? 
  - Can we identify our most valuable clients by predicting its CLV? 
  - What factors tend to make clients decided NOT to renew their subscription?
  - Given the data analysis, what changes should be made in order to retain (and increase) client rate?

URL to dataset: https://www.kaggle.com/datasets/blastchar/telco-customer-churn
>>>>>>> dd47e4fca6172d0766c44b4e3931add5e186d58c

## Maybe add some financials to make the case more interesting
  Fix costs of the infrastructure
  Variable costs of supplying clients with services
  Average revenues per client
  


## Data cleaning

```{r}
library(neuralnet)
library(gmodels)
library(caret)
library(class)
library(corrplot)
library(C50)
library(kernlab) 

clients <- read.csv("ClientsChurn.csv")
str(clients)
summary(clients)
  #Make factors
  #We need to decide how to deal with the NAs in the Total Charges column


clients$customerID <- NULL
  #ID is not needed
clients$gender <- as.factor(clients$gender)
clients$SeniorCitizen <- as.factor(clients$SeniorCitizen)

clients$Partner <- ifelse(clients$Partner == "Yes", 1, 0)
clients$Partner <- as.factor(clients$Partner)

clients$Dependents <- ifelse(clients$Dependents == "Yes", 1, 0)
clients$Dependents <- as.factor(clients$Dependents)

clients$PhoneService <- ifelse(clients$PhoneService == "Yes", 1, 0)
clients$PhoneService <- as.factor(clients$PhoneService)

clients$MultipleLines <- as.factor(clients$MultipleLines)
clients$InternetService <- as.factor(clients$InternetService)
clients$OnlineSecurity <- as.factor(clients$OnlineSecurity)
clients$OnlineBackup <- as.factor(clients$OnlineBackup)
clients$DeviceProtection <- as.factor(clients$DeviceProtection)
clients$TechSupport <- as.factor(clients$TechSupport)
clients$StreamingMovies <- as.factor(clients$StreamingMovies)
clients$StreamingTV <- as.factor(clients$StreamingTV)
clients$Contract <- as.factor(clients$Contract)

clients$PaperlessBilling <- ifelse(clients$PaperlessBilling == "Yes", 1, 0)
clients$PaperlessBilling <- as.factor(clients$PaperlessBilling)

clients$Churn <- ifelse(clients$Churn == "Yes", 1, 0)
clients$Churn <- as.factor(clients$Churn)

clients$PaymentMethod <- as.factor(clients$PaymentMethod)

clients$TotalCharges <- ifelse(is.na(clients$TotalCharges), median(clients$TotalCharges, na.rm = TRUE), clients$TotalCharges)
  #The NA values (a very small number) have been converted to the median

str(clients)
summary(clients)
  #All the variables but the numeric ones are made factors. Binary variables (yes or no) have been converted to 1 and 0

```

## Normalization and test/train dataframes
```{r}
clientss <- as.data.frame(model.matrix(~.-1,clients))
  #Factors are turned into dummy variables

normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

clients_norm <- as.data.frame(lapply(clientss, normalize))
  #Numeric variables are normalized

set.seed(579)
test <- sample(1:nrow(clients_norm), 2000)
  #Test made of 28% of all observations

clients_train <- clients_norm[-test,]
clients_test <- clients_norm[test,]

clients_testlabels <- clients_norm[test, "Churn1"]
clients_trainlabels <- clients_norm[-test, "Churn1"]

  #Train and test data frames created. Labels for future use. 
```

## Buildign a Logistic Regression

```{r}
reg1 <- glm(Churn1 ~ ., data = clients_train, family = "binomial")
reg1 <- step(reg1, direction = "backward")

lrprediction <- predict(reg1, newdata = clients_test, type = "response")
lrprediction <- ifelse(lrprediction > 0.5, 1, 0)

confusionMatrix(as.factor(lrprediction), as.factor(clients_test$Churn1), positive = "1")

  # Kappa = 0.48

lrprediction <- predict(reg1, newdata = clients_test, type = "response")

  #Original values are restored to build the stacked model afterwards
```

## Building a KNN prediction 

```{r}
clients_train$Churn1 <- NULL
clients_test$Churn1 <- NULL

set.seed(579)
KNNprediction <- knn(train = clients_train, test = clients_test,
                      cl = clients_trainlabels, k=sqrt(nrow(clients_train)))

CrossTable(x = clients_testlabels,y= KNNprediction,prop.chisq=FALSE)
confusionMatrix(as.factor(KNNprediction), as.factor(clients_testlabels))

  # Kappa = 0.46

clients_train$Churn1 <- clients_trainlabels
clients_test$Churn1 <- clients_testlabels
  #Churn columns is restored
```

## Neural Net

```{r}
set.seed(579)
neural_model <- neuralnet(Churn1~., data=clients_train)
ANNprediction <- predict(neural_model, newdata = clients_test)

ANNprediction <- ifelse(ANNprediction > 0.5, 1, 0)

CrossTable(x = ANNprediction, y = clients_test$Churn, prop.chisq=FALSE)
confusionMatrix(as.factor(ANNprediction), as.factor(clients_testlabels), positive = "1")

  # Kappa = 0.48

ANNprediction <- predict(neural_model, newdata = clients_test)
  # Original values restored to create the stacked model
```

## Building a Decision Tree

```{r}
treemod <- C5.0(as.factor(Churn1) ~., data = clients_train)
treepred <- predict(treemod, clients_test)

plot(treemod)

confusionMatrix(as.factor(treepred), as.factor(clients_testlabels))
  # Kappa = 0.43
```

## Building a SVM prediction

```{r}
SVM_model <- ksvm(as.factor(Churn1)~., data = clients_train, kernel = "vanilladot")
SVMprediction <- predict(SVM_model, clients_test)

confusionMatrix(as.factor(SVMprediction), as.factor(clients_testlabels))

  #Kappa = 0.47
```

## Buidling the stacked model
```{r}
allpredictions <- data.frame(lrprediction, KNNprediction, ANNprediction, SVMprediction, treepred, clients_testlabels)
summary(allpredictions)

colnames(allpredictions)[colnames(allpredictions) == "clients_testlabels"] ="Churn"
  #Dataset with all 5 predictions and the lables that contain the true values for the test dataset
set.seed(5948)
combined_test_set <- sample(1:nrow(allpredictions), 0.3*nrow(allpredictions)) 

combined_train <- allpredictions[-combined_test_set, ]
combined_test <- allpredictions[combined_test_set,]

combined_dtree <- C5.0(as.factor(Churn) ~., data = combined_train)
plot(combined_dtree)

combined_pred <- predict(combined_dtree, combined_test)
CrossTable(combined_pred, combined_test$Churn)
confusionMatrix(as.factor(combined_pred), as.factor(combined_test$Churn))

#LR and tree, but I almost forced this case with this seed 

# Kappa = 0.41
# Worse than others 
```

## Clustering of clients

```{r}
set.seed(7593)
clients_z <- as.data.frame(lapply(clients_norm, scale))
clients_clusters <- kmeans(clients_norm, 5)

clients_norm$cluster <- clients_clusters$cluster

clients_clusters$centers
  #We also need to interpret this, ask in class
```

## Tuning models

```{r}

```

